- id: ViTMatte
  title: 'ViTMatte: Boosting image matting with pre-trained plain vision transformers'
  authors:
  - Jingfeng Yao
  - Xinggang Wang
  - Shusheng Yang
  - Baoyuan Wang
  publisher: Information Fusion
  date: '2024-03-01'
  link: https://www.sciencedirect.com/science/article/abs/pii/S1566253523004074

- id: VisionMamba
  title: 'Vision mamba: Efficient visual representation learning with bidirectional state space model'
  authors:
  - Lianghui Zhu
  - Bencheng Liao
  - Qian Zhang
  - Xinlong Wang
  - Wenyu Liu
  - Xinggang Wang
  publisher: arXiv preprint
  date: '2024-01-17'
  link: https://arxiv.org/abs/2401.09417
  description: Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models.
  image: https://github.com/hustvl/Vim/raw/main/assets/vim_pipeline_v1.9.png
  buttons:
  - type: repo
    link: https://github.com/hustvl/Vim
  - type: project_page
    link: https://github.com/hustvl/Vim
  # tags:
  # - Vision Foundation Models
